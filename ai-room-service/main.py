# ai-room-service/main.py

import io
import os
from typing import List

import numpy as np
import onnxruntime as ort
import requests
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from PIL import Image

# ========= CONFIG =========

MODEL_PATH = os.path.join("model", "rooms_classifier_finetuned.onnx")

# ‚ö†Ô∏è √Ä ADAPTER selon tes classes r√©elles :
CLASS_NAMES = [
    "bathroom",
    "bedroom",
    "kitchen",
    "living",
    "office"
]


IMG_SIZE = 224  # taille utilis√©e √† l'entra√Ænement (probablement 224x224)


# ========= FASTAPI APP =========

app = FastAPI(
    title="AI Room Type Service",
    description="Microservice pour pr√©dire le type de pi√®ce √† partir d'une image",
    version="1.0.0",
)


class PredictRequest(BaseModel):
    imageUrl: str


class PredictResponse(BaseModel):
    roomType: str
    confidence: float


# ========= MODEL LOADING =========

print("üîÑ Chargement du mod√®le ONNX...")
if not os.path.exists(MODEL_PATH):
    raise RuntimeError(f"Model file not found at {MODEL_PATH}")

session = ort.InferenceSession(
    MODEL_PATH,
    providers=["CPUExecutionProvider"]
)
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name
print("‚úÖ Mod√®le ONNX charg√© !")
print("Input name:", input_name)
print("Output name:", output_name)


# ========= PREPROCESS =========

def download_image(url: str) -> Image.Image:
    """T√©l√©charger l'image √† partir d'une URL et la charger avec PIL."""
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        img_bytes = io.BytesIO(resp.content)
        img = Image.open(img_bytes).convert("RGB")
        return img
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Erreur de t√©l√©chargement de l'image: {e}")


def preprocess_image(img: Image.Image) -> np.ndarray:
    """
    Pr√©traitement de l'image pour matcher le mod√®le ONNX :
    - resize √† IMG_SIZE x IMG_SIZE
    - conversion en numpy
    - normalisation [0,1]
    - mise en forme (1, 3, H, W)
    ‚ö†Ô∏è Si dans ton notebook tu utilisais une normalisation diff√©rente
       (mean/std sp√©cifiques), il faudra l'ajuster ici.
    """
    img = img.resize((IMG_SIZE, IMG_SIZE))
    arr = np.array(img).astype("float32") / 255.0  # [H, W, 3]

    # [H, W, 3] -> [3, H, W]
    arr = np.transpose(arr, (2, 0, 1))

    # batch dimension
    arr = np.expand_dims(arr, axis=0)  # [1, 3, H, W]

    return arr


def softmax(x: np.ndarray) -> np.ndarray:
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=-1, keepdims=True)


# ========= INFERENCE =========

def predict_room_type(image_url: str) -> PredictResponse:
    # 1) T√©l√©charger image
    img = download_image(image_url)

    # 2) Pr√©traitement
    input_tensor = preprocess_image(img)  # [1, 3, 224, 224]

    # 3) Inference ONNX
    outputs = session.run([output_name], {input_name: input_tensor})[0]  # [1, num_classes]

    # 4) Softmax + argmax
    probs = softmax(outputs)[0]  # [num_classes]
    pred_idx = int(np.argmax(probs))
    confidence = float(probs[pred_idx])

    if pred_idx < 0 or pred_idx >= len(CLASS_NAMES):
        room_type = "unknown"
    else:
        room_type = CLASS_NAMES[pred_idx]

    return PredictResponse(roomType=room_type, confidence=confidence)


# ========= ENDPOINTS =========

@app.get("/")
def root():
    return {"message": "AI Room Type Service is running"}


@app.post("/predict", response_model=PredictResponse)
def predict_endpoint(req: PredictRequest):
    """
    Ex: POST /predict
    Body: { "imageUrl": "http://localhost:8086/media/files/xxx.jpg" }
    """
    if not req.imageUrl:
        raise HTTPException(status_code=400, detail="imageUrl is required")

    result = predict_room_type(req.imageUrl)
    return result
